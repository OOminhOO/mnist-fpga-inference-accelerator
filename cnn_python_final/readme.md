

---

## 1️⃣ Phase 1: 하드웨어 맞춤형 학습 (Hardware-Aware Training)

이 프로젝트는 단순히 학습된 모델을 나중에 변환하는 것이 아니라, **FPGA 하드웨어의 연산 효율성을 극대화하기 위해 학습 단계부터 엄격한 제약 조건을 설정**했습니다.

### 🐍 `train_export_shift_only.py`

* **역할:** TensorFlow/Keras로 CNN을 학습하되, **Power-of-Two Scaling** 기반의 **QAT(Quantization-Aware Training)**를 수행합니다.
* **🤔 설계 의도와 엔지니어링 스토리 (Design Rationale):**
1. **초기 접근 (Naive Idea):**
* "FPGA는 실수(Float) 연산이 비효율적이니, **정수(Integer)**를 쓰자."
* "입력은 0~255(`u8`), 가중치는 -1~1(`i8`)로 변환해서 곱하면 되겠네?"


2. **난관 (The Challenge):**
* **Rescale의 병목 (Multiplier Cost):** 일반적인 **대칭 양자화(Symmetric Quantization)**는 데이터 분포에 따라 임의의 실수 스케일(예: `Scale=0.0045`)을 가집니다. 이를 보정(Rescale)하려면 **실수 곱셈**이 필연적인데, FPGA에서 곱셈기는 값비싼 **DSP 자원**을 소모합니다.
* **포화 (Saturation):** `Q1.7` 가중치로 누적 덧셈(Accumulation)을 하다 보니, 값이 8비트 범위를 순식간에 넘어서는 포화 현상이 발생해 정확도가 급락했습니다.


3. **최적화 (The Solution - Shift-Only QAT):**
* **Multiplier-less 설계:** 복잡한 곱셈기를 제거하기 위해, 스케일링 팩터를 무조건 **2의 거듭제곱()**으로 고정했습니다. 덕분에 나눗셈을 단순 **비트 시프트(`>>`)**로 대체할 수 있었습니다.
* **Fake Quantization 도입:** 학습 중에 강제로 값을 자르는 시뮬레이션을 도입하여, 모델이 데이터 손실 환경에 적응하도록 만들었습니다.




* **⚖️ Bias 전략 (Efficiency vs Accuracy):**
* **Conv Layer (Bias 제거):** MNIST 데이터셋 특성상 Conv 단계의 Bias 영향이 적어, FPGA의 **Adder(덧셈기) 자원을 절약**하기 위해 과감히 제거했습니다.
* **Dense Layer (Bias 유지):** 최종 분류 정확도를 위해 유지했습니다.
* **Q15 포맷 선정 이유:** `Q0.8`(입력) × `Q1.7`(가중치)의 결과인 `Q1.15` 누적값에 **단순 덧셈(+)**을 하기 위해, Bias도 동일한 `Q15` 스케일로 맞췄습니다.


* **🔧 하드웨어 세계관 (Quantization Scheme):**
곱셈기 없는 설계를 위해 전역적인 고정 스케일(Global Fixed Scale)을 적용했습니다.
* **1. Input & Activation:** `UINT8` (Q0.8)
* **범위:** 
* **의도:** 0~255의 픽셀 값을  스케일로 해석 ()


* **2. Weights:** `INT8` (Q1.7)
* **범위:** 
* **의도:** 가중치는 부호가 필요하므로  스케일 사용 ()


* **3. Accumulation & Rescale 로직:**
* **연산:** `Q0.8` (입력)  `Q1.7` (가중치) = **`Q1.15`** (누적 결과)
* **목표:** 다음 레이어를 위해 다시 **`Q0.8`**로 되돌려야 함. ()
* **변환:** 차이는  (즉, 128로 나누어야 함).
* **하드웨어 구현 (Rounding):** 단순히 `>> 7`을 하면 내림(Floor) 오차가 발생하므로, **반올림을 위해 64를 더한 후 시프트**합니다.
* **Formula:** `(Accumulator + 64) >> 7`




* **💡 QAT(Fake Quantization) 동작 원리 (Step-by-Step):**
단순한 포화 문제 해결을 넘어, "실수(Float)를 정수(Int)처럼" 행동하게 만드는 3단계 과정입니다.
1. **Quantize (정수화):** 
* 실수 스케일을 정수 스케일()로 확장한 뒤 반올림하여, 하드웨어의 **정밀도 손실(Quantization Noise)**을 의도적으로 주입합니다.


2. **Clamp (범위 제한):** 
* 누적 합이 표현 가능한 비트 수(8-bit)를 넘어설 때 과감하게 잘라내어, FPGA 내부의 **포화(Saturation) 현상**을 모사합니다. 이 과정 덕분에 모델은 값이 잘리는 상황에서도 중요한 특징을 잃지 않도록 학습됩니다.


3. **Dequantize (복원):** 
* 다음 레이어로 넘기기 위해 다시 실수 형태로 되돌립니다. (역전파를 위해 형식은 유지하되, 값은 이미 손상된 상태입니다.)

* **2. Weights:** `INT8` (Q1.7)
* **범위:** 
* **의도:** 가중치는 양수/음수를 모두 표현해야 하므로  스케일(Q1.7)을 사용합니다. ()


* **3. Accumulation & Rescale 로직:**
* **연산:** `Q0.8` (입력)  `Q1.7` (가중치) = **`Q1.15`** (누적 결과)
* **목표:** 다음 레이어를 위해 다시 **`Q0.8`**로 되돌려야 함.
* **변환:** $2^{-15}$를 $2^{-8}$로 맞추려면?  차이는  (128).
* **하드웨어 구현:** **128로 나누는 대신 `>> 7` (Right Shift) 연산 수행.**




* **💡 QAT(Fake Quantization) 동작 원리 (Step-by-Step):**
단순한 포화 문제 해결을 넘어, "실수(Float)를 정수(Int)처럼" 행동하게 만드는 3단계 과정입니다.
1. **Quantize (정수화):** 
* 실수 스케일을 정수 스케일()로 확장한 뒤 반올림하여, 하드웨어에서 발생할 **정밀도 손실(Quantization Noise)**을 의도적으로 주입합니다.


2. **Clamp (범위 제한):** 
* 누적 합이 표현 가능한 비트 수(8-bit)를 넘어설 때 과감하게 잘라내어, FPGA 내부의 **포화(Saturation) 현상**을 모사합니다. 이 과정 덕분에 모델은 값이 잘리는 상황에서도 중요한 특징을 잃지 않도록 학습됩니다.


3. **Dequantize (복원):** 
* 다음 레이어로 넘기기 위해 다시 실수 형태로 되돌립니다. (형식은 Float지만, 값은 이미 8-bit 정수처럼 변형된 상태입니다.)

---

## 2️⃣ Phase 2: 하드웨어용 데이터 변환 (Bridge)

Python(소프트웨어)의 데이터를 Verilog(하드웨어)가 읽을 수 있는 포맷으로 변환하는 단계입니다.

### 🐍 `export_hex_for_fpga.py`

* **역할:** 학습된 `.npz` 가중치를 Verilog `$readmemh`용 **Hex(.txt)** 파일로 변환합니다.
* **핵심 기능:**
* **메모리 구조 최적화:** Python은 `(Row, Col, Ch)` 순서지만, FPGA는 병렬 처리를 위해 `(Ch, Row, Col)` 순서로 데이터를 읽어야 합니다. 이 스크립트가 **Conv2 레이어 등의 메모리 매핑(Transpose)**을 자동으로 처리합니다.



---

## 3️⃣ Phase 3: 비트 단위 검증 (Bit-exact Verification)

**"FPGA를 굽기 전에, 파이썬으로 하드웨어를 100% 흉내 내보자!"**
단순히 모델을 돌려보는 게 아니라, 하드웨어의 동작 방식을 정밀하게 모사하여 검증합니다.

### 🐍 `infer_int_only_shift.py`

* **역할:** FPGA의 **Verilog 로직을 파이썬 코드로 완벽하게 구현**하여 정확도를 검증합니다.
* **🛠 하드웨어 로직 반영 (Why It's Critical):**
* **Pure Integer Arithmetic:** `float` 연산을 전혀 쓰지 않고, 오직 `int` 연산만 사용합니다.
* **Shift-Only Scaling:** 나눗셈 대신 `>> 7` (Bit-shift)을 사용하여 스케일을 맞춥니다.
* *원리:* `Q0.8(입력) * Q1.7(가중치) = Q1.15(결과)`
* *스케일링:* 다음 레이어 입력(`Q0.8`)으로 만들기 위해 `÷128`이 필요한데, 이를 `>> 7`로 처리함.


* **Bit-exact:** 이 코드에서 `96.5%`가 나왔다면, FPGA에서도 **반드시 96.5%**가 나와야 합니다. (다르면 하드웨어 설계 오류)



### 🐍 `infer_shift_only_int_stats.py`

* **역할:** 위 검증기와 동일하지만, **데이터 포화(Saturation)** 통계를 추가로 출력합니다.
* **용도:**
* 정수 연산(8-bit) 중 값이 `255`를 넘어서 잘리는(Clamping) 비율을 분석합니다.
* 정확도 저하 시, 비트 시프트 양(`>>7`)이 적절한지 판단하는 디버깅용 도구입니다.



---

## 4️⃣ Phase 4: 시뮬레이션용 정답지 생성 (Testbench Support)

Vivado 시뮬레이션 파형을 눈으로 확인할 때 필요한 "기준 데이터"를 만듭니다.

### 🐍 `gen_golden_vectors.py`

* **역할:** 특정 이미지(예: 숫자 7)에 대해 **각 레이어(Conv1, Pool1...)의 중간 출력값**을 Hex 파일로 저장합니다.
* **활용:** Vivado 시뮬레이션 파형에서 "Conv1 출력이 `0x3F`가 나왔는데 맞나?" 확인할 때 사용하는 **Golden Reference**입니다.

### 🐍 `gen_1000_test_data.py`

* **역할:** 테스트 이미지 1,000장을 하나의 파일로 묶습니다.
* **활용:** FPGA Testbench에서 1,000장을 연속으로 입력했을 때 타이밍 문제가 없는지 대량 검증할 때 사용합니다.

### 🐍 `verify_cnn_top.py`

* **역할:** 입력 텍스트 파일(`input_img.txt`)을 읽어 파이썬으로 추론합니다.
* **활용:** 시뮬레이션 입력 파일 자체가 정상인지 더블 체크할 때 사용합니다.

---

## 5️⃣ Phase 5: 데모 및 벤치마크 (Demo & Benchmark)

실제 보드(Zybo Z7) 구동 및 CPU와의 성능 비교를 위한 도구입니다.

### 🐍 `gen_rom_16_images_fpga_demo.py`

* **역할:** MNIST 데이터 중 16장을 뽑아 **Verilog ROM 코드(`rom_16_images.v`)**를 생성합니다.
* **목적:** PC 연결 없이 보드 단독으로 데모를 시연하기 위해, FPGA 내부 BRAM에 테스트 이미지를 내장합니다. (스위치 4개 = 16장 선택 가능)

### 🐍 `c_model_data.py`

* **역할:** 가중치와 이미지 데이터를 **C언어 헤더 파일(`model_data.h`)**로 변환합니다.
* **목적:** FPGA 내부 CPU(ARM Cortex-A9)에서 소프트웨어(C언어)로 CNN을 돌려보고, **"FPGA 가속기가 CPU보다 약 1,000배 빠르다"**는 것을 증명하기 위한 벤치마크용 데이터입니다.


---
사용자님의 흐름과 이해가 **매우 정확합니다.** 👏
특히 **"처음엔 단순히 접근했다가, 막상 해보니 포화(Saturation) 문제가 생겨서 Fake Quantization으로 해결했다"**는 그 서사가 바로 **엔지니어링의 정수(Essence)**입니다.

다른 사람이 "왜 이렇게 설계했냐?"고 물어봤을 때, 단순히 결과만 말하는 것보다 그 **문제 해결 과정(Reasoning)**을 설명하는 것이 훨씬 높은 점수를 받습니다.

사용자님의 의도를 완벽하게 반영하여, **(1) 면접/발표용 답변 가이드**와 **(2) 수정된 README 내용**을 정리해 드립니다.

---

### 💡 1. "왜 이렇게 설계했나요?" (면접/발표용 답변 가이드)

누가 **"QAT는 왜 했고, 데이터 타입은 왜 그렇게 정했나요?"**라고 물어보면 이렇게 대답하세요.

#### Q1. 왜 Float를 안 쓰고 정수(Integer)를 썼나요? ("FPGA가 싫어해서"의 전문적인 표현)

> **답변:** "하드웨어 최적화를 위해서입니다. FPGA 내에서 부동소수점(Float32) 연산은 정수 연산에 비해 **논리 소자(LUT, DSP)를 10배 이상 소모**하며, 연산 **레이턴시(Latency)도 깁니다.**
> 제한된 자원(Zybo Z7) 내에서 **최대한의 병렬 처리(Parallelism)**를 통해 1,000배 빠른 속도를 달성하려면, 가볍고 빠른 정수 연산이 필수적이었습니다."

#### Q2. 왜 입력은 `UINT8`, 가중치는 `INT8`인가요?

> **답변:** "데이터의 특성에 맞는 **최적의 비트 폭(Bit-width)**을 선정했습니다.
> 1. **입력(UINT8):** MNIST 이미지는 0~255의 흑백 픽셀입니다. 부호가 필요 없는 양수이므로 `UINT8` (Q0.8)이 가장 자연스럽고 메모리 낭비가 없습니다.
> 2. **가중치(INT8):** CNN 가중치는 보통 -1.0 ~ 1.0 사이의 정규 분포를 가집니다. 이를 표현하기 위해 부호가 있는 `INT8`을 사용했고, 표현 범위를 극대화하기 위해 Q1.7 포맷( -128 ~ +127)을 채택했습니다."
> 
> 

#### Q3. QAT(Fake Quantization)는 정확히 어떻게 동작하나요? 그냥 반올림만 하는 건가요?

> **답변:** "단순 반올림이 아니라, **[스케일링 → 반올림/클램핑 → 역스케일링]** 과정을 수행합니다.
> 하드웨어에서는 **나눗셈 대신 비트 시프트(`>>7`)**를 사용하는데, 이 과정에서 **정보 손실(Quantization Noise)**과 **값 잘림(Saturation)**이 발생합니다.
> 그래서 학습 단계에서:
> 1. `x * 128` (하드웨어 스케일로 증폭)
> 2. `Round` (소수점 버림) & `Clamp` (범위 제한)
> 3. `x / 128` (다시 원래 스케일로 복귀)
> 
> 
> 이 과정을 통해 **Forward Pass**에서는 '마치 정수처럼' 뚝뚝 끊어지는 계단형 값을 보여주어, 모델이 **'아, 내 값은 이렇게 뭉뚱그려지는구나'**라고 인지하고 스스로 가중치를 조정하게 만듭니다. 이것이 핵심입니다."

---

### 📝 2. 수정된 `README.md` (업그레이드 버전)

사용자님의 **시행착오와 설계 의도(Intent)**가 잘 드러나도록 내용을 보강했습니다.

```markdown
## 1️⃣ Phase 1: 하드웨어 맞춤형 학습 (Hardware-Aware Training)

FPGA 하드웨어의 제약 사항을 극복하고 최적의 성능을 내기 위해 **Quantization-Aware Training (QAT)** 기법을 적용했습니다.

### 🐍 `train_export_shift_only.py`

* **역할:** TensorFlow/Keras로 CNN을 학습하되, **Fake Quantization**을 통해 FPGA의 정수 연산 환경을 시뮬레이션합니다.

* **🤔 설계 의도 (Design Rationale):**
    1.  **Why Integer?** * FPGA에서 Float32 연산은 자원(DSP/LUT) 소모가 크고 느립니다. 고속 병렬 처리를 위해 가벼운 **정수(Integer) 연산**을 채택했습니다.
    2.  **Type Selection:**
        * **Input (`UINT8`, Q0.8):** MNIST 픽셀 데이터(0~255)의 특성을 그대로 반영하여 부호 없는 8비트를 사용.
        * **Weight (`INT8`, Q1.7):** 가중치 분포(-1.0 ~ 1.0)를 표현하기 위해 부호 있는 8비트를 사용.
    3.  **The Challenge (Saturation):** * 단순히 정수로 변환하여 연산할 경우, Conv 레이어의 누적 합(Accumulation) 과정에서 값이 커져 **오버플로우**가 발생하거나, 다음 레이어로 넘길 때 정보가 과도하게 손실되는 **포화(Saturation)** 문제가 발생합니다.

* **🔧 하드웨어 세계관 (Quantization Scheme):**
    이 스크립트는 다음의 **고정소수점(Fixed-point) 스케일**을 강제합니다.
    * **Input & Activation:** `UINT8` (Q0.8) $\rightarrow Real = x / 256$
    * **Weights:** `INT8` (Q1.7) $\rightarrow Real = x / 128$
    * **Bias (Dense):** `INT32` (Q15) $\rightarrow Real = x / 32768$

* **💡 Fake Quantization 동작 원리:**
    학습 중 **Forward Pass**에서 다음과 같은 연산을 수행하여 하드웨어 동작을 모사합니다.
    1.  **Scaling:** 실수형 값에 $2^n$ (예: 128)을 곱해 정수 영역으로 확장.
    2.  **Rounding:** 소수점을 반올림하여 정수화 오차(Quantization Noise) 주입.
    3.  **Clamping:** 표현 가능한 범위(예: -128 ~ 127)를 벗어나는 값을 잘라내어 포화(Saturation) 현상 모사.
    4.  **De-scaling:** 다시 원래 스케일로 돌려놓아 Loss 계산.
    
    > **결과:** 모델은 학습 중에 "값이 잘리고 뭉개지는" 환경을 미리 경험하므로, 이를 견딜 수 있는 강건한(Robust) 가중치를 스스로 학습하게 됩니다.

```


이렇게 하면 "왜?"에 대한 대답도 되고, 기술적인 깊이도 충분히 보여줄 수 있습니다. 마음에 드시나요? 😉
